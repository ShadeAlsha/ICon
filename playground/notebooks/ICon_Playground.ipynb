{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I-Con Playground\n",
    "\n",
    "**A Student-Friendly Tool for Exploring I-Con Representation Learning**\n",
    "\n",
    "---\n",
    "\n",
    "## What is I-Con?\n",
    "\n",
    "**I-Con (Integrated Contrastive learning)** is a unifying framework for representation learning that shows how many seemingly different methods are actually optimizing the same underlying objective: minimizing the KL divergence between two neighborhood distributions.\n",
    "\n",
    "This framework unifies over 20 methods including:\n",
    "- **Dimensionality reduction**: SNE, t-SNE\n",
    "- **Contrastive learning**: SimCLR, InfoNCE\n",
    "- **Clustering**: k-means, spectral clustering\n",
    "- **Supervised learning**: Cross-entropy classification\n",
    "\n",
    "The key insight is that all these methods can be written as:\n",
    "\n",
    "$$\\mathcal{L} = D_{KL}(P \\| Q)$$\n",
    "\n",
    "Where:\n",
    "- $P$ is the **supervisory distribution** (defines which points should be neighbors)\n",
    "- $Q$ is the **learned distribution** (defines similarity in the embedding space)\n",
    "\n",
    "---\n",
    "\n",
    "## About This Notebook\n",
    "\n",
    "This notebook is a **playground** for running small I-Con experiments on datasets like CIFAR-10 and MNIST. It is designed for:\n",
    "- **Learning**: Understand how different I-Con configurations affect learned representations\n",
    "- **Experimentation**: Quickly try different backbones, objectives, and hyperparameters\n",
    "- **Visualization**: See how embeddings cluster and separate by class\n",
    "\n",
    "**Note**: This is NOT designed for reproducing the full I-Con paper experiments (e.g., ImageNet-scale training). For that, refer to the main I-Con repository documentation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules from the playground."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directories to the path (run this if importing fails)\n",
    "import sys\n",
    "sys.path.insert(0, '../..')  # Add root of I-Con repo\n",
    "sys.path.insert(0, '..')     # Add playground directory\n",
    "\n",
    "# Core playground imports\n",
    "from playground.playground_config import PlaygroundConfig, quick_config\n",
    "from playground.playground_runner import run_playground_experiment, load_experiment_results\n",
    "from playground.playground_viz import plot_training_curves, plot_embeddings_2d, create_experiment_summary\n",
    "from playground.playground_probes import run_linear_probe, run_toy_negation_probe, analyze_embedding_separability\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nicer notebook display\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [10, 6]\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print(\"Imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Configure Your Experiment\n",
    "\n",
    "The `PlaygroundConfig` class lets you easily set up an I-Con experiment. Here are the key parameters:\n",
    "\n",
    "| Parameter | Description | Options |\n",
    "|-----------|-------------|----------|\n",
    "| `dataset` | Dataset to train on | `cifar10`, `cifar100`, `mnist`, `stl10` |\n",
    "| `backbone` | Encoder architecture | `resnet18`, `resnet34`, `resnet50`, `simplecnn`, `mlp` |\n",
    "| `icon_mode` | I-Con objective preset | `simclr_like`, `sne_like`, `tsne_like`, `supervised`, `cluster_like` |\n",
    "| `epochs` | Training epochs | Any positive integer (start small, e.g., 5-20) |\n",
    "| `batch_size` | Batch size | 64, 128, 256, etc. |\n",
    "| `temperature` | Softmax temperature | 0.1 - 1.0 (lower = sharper) |\n",
    "| `embedding_dim` | Size of learned embeddings | 32, 64, 128, 256, etc. |\n",
    "\n",
    "### I-Con Mode Descriptions\n",
    "\n",
    "- **`simclr_like`**: Contrastive learning (InfoNCE-style) - learns by pulling augmented views together\n",
    "- **`sne_like`**: SNE-style embedding with Gaussian kernel in embedding space\n",
    "- **`tsne_like`**: t-SNE-style with Student-t kernel (heavier tails, better for visualization)\n",
    "- **`supervised`**: Uses class labels directly as the supervisory signal\n",
    "- **`cluster_like`**: Clustering-oriented with learnable temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration for your experiment\n",
    "# Feel free to modify these parameters!\n",
    "\n",
    "config = PlaygroundConfig(\n",
    "    # Dataset and model\n",
    "    dataset=\"cifar10\",           # Try: \"cifar10\", \"mnist\", \"cifar100\"\n",
    "    backbone=\"resnet18\",         # Try: \"resnet18\", \"resnet50\" (use \"simplecnn\" for MNIST)\n",
    "    icon_mode=\"simclr_like\",     # Try: \"simclr_like\", \"tsne_like\", \"supervised\"\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    epochs=10,                   # Start small (5-10), increase if needed\n",
    "    batch_size=256,              # Reduce if you run out of memory\n",
    "    learning_rate=1e-3,          # Standard starting point\n",
    "    temperature=0.5,             # Lower = sharper distributions\n",
    "    embedding_dim=128,           # Size of the representation\n",
    "    \n",
    "    # Output location\n",
    "    output_dir=\"playground_runs\",\n",
    ")\n",
    "\n",
    "# Print the configuration summary\n",
    "print(config.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative: Use a Preset\n",
    "\n",
    "If you just want to get started quickly, you can use a preset configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment one of these to use a preset instead:\n",
    "\n",
    "# config = quick_config(\"cifar_contrastive\", epochs=10)  # SimCLR-like on CIFAR-10\n",
    "# config = quick_config(\"cifar_supervised\", epochs=10)   # Supervised on CIFAR-10  \n",
    "# config = quick_config(\"mnist_tsne\", epochs=10)         # t-SNE-like on MNIST\n",
    "\n",
    "# print(config.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Run the Experiment\n",
    "\n",
    "Now let's train the model! This will:\n",
    "1. Load the dataset\n",
    "2. Initialize the encoder and I-Con objective\n",
    "3. Train for the specified number of epochs\n",
    "4. Extract embeddings from the validation set\n",
    "5. Save everything to the output directory\n",
    "\n",
    "**Note**: Training time depends on your hardware. With a GPU, CIFAR-10 with ResNet-18 for 10 epochs should take 5-15 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment!\n",
    "# Set verbose=True to see progress, verbose=False for less output\n",
    "\n",
    "results = run_playground_experiment(\n",
    "    config,\n",
    "    verbose=True,\n",
    "    gpu=True,  # Set to False if you don't have a GPU\n",
    ")\n",
    "\n",
    "print(f\"\\nExperiment complete! Results saved to: {results['paths']['run_dir']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Visualize Training Progress\n",
    "\n",
    "Let's look at how the loss evolved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig = plot_training_curves(\n",
    "    results[\"logs\"],\n",
    "    figsize=(12, 4),\n",
    "    show=True,\n",
    ")\n",
    "\n",
    "# Print final metrics\n",
    "if results[\"logs\"][\"val_losses\"]:\n",
    "    print(f\"Final validation loss: {results['logs']['val_losses'][-1]:.4f}\")\n",
    "if results[\"logs\"][\"val_accuracies\"]:\n",
    "    print(f\"Final validation accuracy: {results['logs']['val_accuracies'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the Learned Embeddings\n",
    "\n",
    "One of the most interesting things to look at is how the learned representations organize samples in space. We'll use dimensionality reduction (PCA or t-SNE) to project the high-dimensional embeddings to 2D.\n",
    "\n",
    "**What to look for:**\n",
    "- Do points of the same class cluster together?\n",
    "- How well separated are different classes?\n",
    "- Are there any interesting sub-clusters within classes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with PCA (fast)\n",
    "print(f\"Embedding shape: {results['embeddings'].shape}\")\n",
    "print(f\"Number of samples: {len(results['labels'])}\")\n",
    "print(f\"Number of classes: {len(np.unique(results['labels']))}\")\n",
    "\n",
    "fig = plot_embeddings_2d(\n",
    "    results[\"embeddings\"],\n",
    "    results[\"labels\"],\n",
    "    method=\"pca\",  # Fast! Use \"tsne\" for nicer (but slower) visualization\n",
    "    title=f\"Learned Embeddings ({config.icon_mode})\",\n",
    "    figsize=(10, 8),\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Visualize with t-SNE (takes longer but often looks nicer)\n",
    "# Uncomment the lines below to run\n",
    "\n",
    "# fig = plot_embeddings_2d(\n",
    "#     results[\"embeddings\"],\n",
    "#     results[\"labels\"],\n",
    "#     method=\"tsne\",\n",
    "#     title=f\"Learned Embeddings ({config.icon_mode}) - t-SNE\",\n",
    "#     max_samples=2000,  # Limit samples for speed\n",
    "#     perplexity=30,\n",
    "#     show=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Evaluate with Linear Probe\n",
    "\n",
    "A **linear probe** trains a simple logistic regression classifier on top of the frozen embeddings. This is a standard way to evaluate representation quality:\n",
    "\n",
    "- **High accuracy** = embeddings are linearly separable by class = good representations\n",
    "- **Low accuracy** = class information is not easily accessible = embeddings may need more training or different objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear probe evaluation\n",
    "probe_results = run_linear_probe(\n",
    "    results[\"embeddings\"],\n",
    "    results[\"labels\"],\n",
    "    test_size=0.2,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\nLinear probe test accuracy: {probe_results['test_accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze embedding separability\n",
    "# This shows how well different classes are separated in the embedding space\n",
    "\n",
    "sep_results = analyze_embedding_separability(\n",
    "    results[\"embeddings\"],\n",
    "    results[\"labels\"],\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Create a Summary\n",
    "\n",
    "Let's create a summary figure that shows everything at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary figure\n",
    "fig = create_experiment_summary(\n",
    "    results,\n",
    "    show=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Bonus: Toy Negation Probe (Pedagogical)\n",
    "\n",
    "This is a simple demonstration of **\"affirmation bias\"** - a phenomenon where embedding models often place semantically opposite sentences close together due to lexical overlap.\n",
    "\n",
    "For example:\n",
    "- \"The dog is jumping\" and \"The dog is NOT jumping\" share many words\n",
    "- Naive similarity metrics will score them as very similar\n",
    "- But they have opposite meanings!\n",
    "\n",
    "**Note**: This is a TOY example for educational purposes. For rigorous evaluation of negation understanding, see benchmarks like NegBench."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the toy negation probe\n",
    "negation_results = run_toy_negation_probe(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercises for Students\n",
    "\n",
    "Now that you've run one experiment, try these exercises to deepen your understanding:\n",
    "\n",
    "### Exercise 1: Compare I-Con Modes\n",
    "Run experiments with different `icon_mode` settings and compare:\n",
    "- How do the embeddings look different between `simclr_like` and `supervised`?\n",
    "- Which mode gives better linear probe accuracy?\n",
    "\n",
    "### Exercise 2: Effect of Temperature\n",
    "Try different `temperature` values (0.1, 0.5, 1.0, 2.0) and observe:\n",
    "- How does temperature affect training stability?\n",
    "- How does it affect the clustering in embedding visualizations?\n",
    "\n",
    "### Exercise 3: Embedding Dimension\n",
    "Compare different `embedding_dim` values (32, 128, 512):\n",
    "- Does a larger embedding dimension always help?\n",
    "- How does it affect the linear probe accuracy?\n",
    "\n",
    "### Exercise 4: Different Datasets\n",
    "Try MNIST with `simplecnn` backbone:\n",
    "- How does the representation differ from CIFAR-10?\n",
    "- Is it easier or harder to learn good embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Space for your experiments!\n",
    "\n",
    "# Example: Try a different configuration\n",
    "# config_new = PlaygroundConfig(\n",
    "#     dataset=\"mnist\",\n",
    "#     backbone=\"simplecnn\",\n",
    "#     icon_mode=\"tsne_like\",\n",
    "#     epochs=10,\n",
    "#     temperature=1.0,\n",
    "# )\n",
    "# results_new = run_playground_experiment(config_new, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Loading Previous Experiments\n",
    "\n",
    "If you've already run experiments and want to load them later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a previous experiment\n",
    "# Replace with the actual path to your experiment\n",
    "\n",
    "# from playground.playground_runner import load_experiment_results, list_experiments\n",
    "\n",
    "# # List all saved experiments\n",
    "# experiments = list_experiments(\"playground_runs\")\n",
    "# for exp in experiments:\n",
    "#     print(f\"{exp['name']}: {exp['dataset']} / {exp['icon_mode']} ({exp['epochs']} epochs)\")\n",
    "\n",
    "# # Load a specific experiment\n",
    "# loaded_results = load_experiment_results(\"playground_runs/YOUR_EXPERIMENT_NAME\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **I-Con Paper**: \"I-Con: A Unifying Framework for Representation Learning\"\n",
    "- **SimCLR**: Chen et al., \"A Simple Framework for Contrastive Learning of Visual Representations\"\n",
    "- **t-SNE**: van der Maaten & Hinton, \"Visualizing Data using t-SNE\"\n",
    "\n",
    "---\n",
    "\n",
    "*This playground is an educational layer on top of the official I-Con implementation. All core I-Con logic comes from the original authors.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
